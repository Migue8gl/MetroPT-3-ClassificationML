{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv(\"../data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervalos an칩malos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "anomalies = [\n",
    "    (datetime.datetime(2020, 4, 12, 11, 50), datetime.datetime(2020, 4, 12, 23, 30)),\n",
    "    (datetime.datetime(2020, 4, 18, 0, 0), datetime.datetime(2020, 4, 18, 23, 59)),\n",
    "    (datetime.datetime(2020, 4, 19, 0, 0), datetime.datetime(2020, 4, 19, 1, 30)),\n",
    "    (datetime.datetime(2020, 4, 29, 3, 20), datetime.datetime(2020, 4, 29, 4, 0)),\n",
    "    (datetime.datetime(2020, 4, 29, 22, 0), datetime.datetime(2020, 4, 29, 22, 20)),\n",
    "    (datetime.datetime(2020, 5, 13, 14, 0), datetime.datetime(2020, 5, 13, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 18, 5, 0), datetime.datetime(2020, 5, 18, 5, 30)),\n",
    "    (datetime.datetime(2020, 5, 19, 10, 10), datetime.datetime(2020, 5, 19, 11, 0)),\n",
    "    (datetime.datetime(2020, 5, 19, 22, 10), datetime.datetime(2020, 5, 19, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 20, 0, 0), datetime.datetime(2020, 5, 20, 20, 0)),\n",
    "    (datetime.datetime(2020, 5, 23, 9, 50), datetime.datetime(2020, 5, 23, 10, 10)),\n",
    "    (datetime.datetime(2020, 5, 29, 23, 30), datetime.datetime(2020, 5, 29, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 30, 0, 0), datetime.datetime(2020, 5, 30, 6, 0)),\n",
    "    (datetime.datetime(2020, 6, 1, 15, 0), datetime.datetime(2020, 6, 1, 15, 40)),\n",
    "    (datetime.datetime(2020, 6, 3, 10, 0), datetime.datetime(2020, 6, 3, 11, 0)),\n",
    "    (datetime.datetime(2020, 6, 5, 10, 0), datetime.datetime(2020, 6, 5, 23, 59)),\n",
    "    (datetime.datetime(2020, 6, 6, 0, 0), datetime.datetime(2020, 6, 6, 23, 59)),\n",
    "    (datetime.datetime(2020, 6, 7, 0, 0), datetime.datetime(2020, 6, 7, 14, 30)),\n",
    "    (datetime.datetime(2020, 7, 8, 17, 30), datetime.datetime(2020, 7, 8, 19, 0)),\n",
    "    (datetime.datetime(2020, 7, 15, 14, 30), datetime.datetime(2020, 7, 15, 19, 0)),\n",
    "    (datetime.datetime(2020, 7, 17, 4, 30), datetime.datetime(2020, 7, 17, 5, 30)),\n",
    "]\n",
    "\n",
    "\n",
    "raros = [\n",
    "    #NO APARECE COMO ANOMALIA\n",
    "    (datetime.datetime(2020, 3, 6, 21, 42, 15), datetime.datetime(2020, 3, 6, 23, 14, 0)),\n",
    "    (datetime.datetime(2020, 3, 11, 5, 15, 10), datetime.datetime(2020, 3, 11, 6, 25, 0)),\n",
    "    (datetime.datetime(2020, 3, 12, 0, 15, 56), datetime.datetime(2020, 3, 12, 11, 59, 0)),\n",
    "    (datetime.datetime(2020, 3, 26, 4, 0, 20), datetime.datetime(2020, 3, 26, 5, 20, 0)),\n",
    "    (datetime.datetime(2020, 3, 27, 7, 12, 0), datetime.datetime(2020, 3, 27, 12, 1, 0)),\n",
    "    (datetime.datetime(2020, 4, 17, 8, 50, 28), datetime.datetime(2020, 4, 17, 23, 59, 0)),\n",
    "    (datetime.datetime(2020, 4, 25, 0, 7, 15), datetime.datetime(2020, 4, 25, 1, 10, 0)),\n",
    "    (datetime.datetime(2020, 5, 19, 1, 35, 28), datetime.datetime(2020, 5, 19, 2, 40, 0)),\n",
    "    (datetime.datetime(2020, 6, 12, 1, 41, 7), datetime.datetime(2020, 6, 12, 17, 6, 0)),\n",
    "    (datetime.datetime(2020, 7, 21, 13, 32, 48), datetime.datetime(2020, 7, 21, 22, 3, 0)),\n",
    "    (datetime.datetime(2020, 7, 22, 6, 40, 46), datetime.datetime(2020, 7, 22, 13, 10, 0)),\n",
    "    (datetime.datetime(2020, 7, 31, 0, 57, 33), datetime.datetime(2020, 7, 31, 2, 9, 0))\n",
    "]\n",
    "\n",
    "anomalies.extend(raros)\n",
    "\n",
    "# Correci칩n de periodos temporales.\n",
    "final_anomalies = []\n",
    "for anomaly in anomalies: \n",
    "  new = list(anomaly)\n",
    "  if anomaly[1].minute == 59:\n",
    "    new[1] += datetime.timedelta(minutes=1)\n",
    "  final_anomalies.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df = df.with_columns(pl.col(\"timestamp\").str.to_datetime())\n",
    "df = df.with_row_index(\"rownr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies[1][1] + datetime.timedelta(seconds=59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos las anomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anomaly(instance_date, anomalies: list[datetime]):\n",
    "    flag_anomaly = False\n",
    "    index = 0\n",
    "    while not flag_anomaly and index < len(anomalies):\n",
    "        t = anomalies[index]\n",
    "        if instance_date >= t[0] and instance_date <= t[1] + datetime.timedelta(seconds=59):\n",
    "            flag_anomaly = True\n",
    "        index += 1\n",
    "    return flag_anomaly\n",
    "\n",
    "\n",
    "df = df.select(pl.all(), pl.lit(0).alias(\"is_anomaly\"))\n",
    "for anomaly in final_anomalies:\n",
    "  df = df.with_columns((pl.col(\"is_anomaly\") + df[\"timestamp\"].is_between(anomaly[0], anomaly[1])))\n",
    "df = df.select(pl.exclude(\"is_anomaly\"), pl.col(\"is_anomaly\") >= 1)\n",
    "# df = df.select(pl.all(), pl.col(\"timestamp\").map_elements(lambda x: is_anomaly(x, anomalies), return_dtype=pl.Boolean).alias(\"is_anomaly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justificamos el c치lculo de la ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transition states\n",
    "tdf = df.select(pl.all(), pl.col(\"is_anomaly\").shift(-1).alias(\"next_is_anomaly\"), pl.col(\"Motor_current\").gt(0.05).alias(\"motor_state\"))\n",
    "tdf = tdf.select(pl.all(), (pl.col(\"is_anomaly\") != pl.col(\"next_is_anomaly\")).alias(\"transition\"))\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create groups based on transition\n",
    "tdf = tdf.select(pl.all(), pl.col(\"transition\").cum_sum().alias(\"groups\"))\n",
    "tdf[\"groups\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = tdf.select(pl.all(), pl.col(\"groups\").shift(-1).alias(\"next_group\"), pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\"))\n",
    "# Filter a ON switch\n",
    "fdf = gdf.filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1)).with_columns(pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\"))\n",
    "# Filter out different time groups\n",
    "fdf = fdf.filter(pl.col(\"groups\") == pl.col(\"next_group\"))\n",
    "# Calculate the duration\n",
    "fdf = fdf.with_columns((pl.col(\"next_timestamp\") - pl.col(\"timestamp\")).alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "print(fdf[\"duration\"].dt.total_seconds().max(), fdf[\"duration\"].dt.total_seconds().min(), fdf[\"duration\"].dt.total_seconds().mean(), fdf[\"duration\"].dt.total_seconds().mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.filter(pl.col(\"duration\").eq(pl.col(\"duration\").max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "data_anomaly = df.filter(pl.col(\"timestamp\").is_between(\n",
    "datetime.datetime(2020,6,5,00,00,00), datetime.datetime(2020,6,8,15,00,00)\n",
    "))\n",
    "px.line(data_anomaly, x=\"timestamp\", y=[\"TP3\", \"Motor_current\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anomaly.select(pl.col(\"is_anomaly\"), pl.col(\"is_anomaly\").shift(-1).alias(\"next_is_anomaly\"), pl.col(\"timestamp\"), pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\")).filter(~pl.col(\"is_anomaly\").eq(pl.col(\"next_is_anomaly\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cada grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = tdf.select(pl.all(), pl.col(\"groups\").shift(-1).alias(\"next_group\"), pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\"))\n",
    "# Filter a ON switch\n",
    "durations = []\n",
    "for label, group in gdf.filter(pl.col(\"groups\") == pl.col(\"next_group\")).group_by(\"groups\"):\n",
    "  tmp = group.filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1)).with_columns(pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\"))\n",
    "  tmp = tmp.with_columns((pl.col(\"next_timestamp\") - pl.col(\"timestamp\")).alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  durations += tmp[\"duration\"].to_list()\n",
    "durations = [x.total_seconds() for x in durations]\n",
    "print(max(durations), min(durations), sum(durations) / len(durations), durations[len(durations)//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para el primer tramo no-an칩malo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.filter(pl.col(\"timestamp\").lt(anomalies[0][0]))\n",
    "subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))[\"timestamp\"].diff().dt.total_seconds().mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cada grupo evitando saltos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "subdf = df.filter(pl.col(\"timestamp\").lt(final_anomalies[0][0]))\n",
    "subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "print(len(durations), len(subdf))\n",
    "durations += subdf[\"duration\"].to_list()\n",
    "for anomaly, next_anomaly, next_next_anomaly in zip(final_anomalies, final_anomalies[1:], final_anomalies[2:]):\n",
    "  subdf = df.filter(\n",
    "    (pl.col(\"timestamp\").lt(next_anomaly[0]) & (pl.col(\"timestamp\").gt(anomaly[1])))\n",
    "  )\n",
    "  subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "  subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "  subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  print(len(durations), len(subdf))\n",
    "  durations += subdf[\"duration\"].to_list()\n",
    "  subdf = df.filter(\n",
    "    (pl.col(\"timestamp\").gt(next_anomaly[1]) & (pl.col(\"timestamp\").lt(next_next_anomaly[0])))\n",
    "  )\n",
    "  subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "  subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "  subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  print(len(durations), len(subdf))\n",
    "  durations += subdf[\"duration\"].to_list()\n",
    "print(max(durations), min(durations), sum(durations) / len(durations), sorted(durations)[len(durations)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))\n",
    "df = df.with_columns((pl.col(\"diff\").dt.total_seconds() // (1249 * 2)).alias(\"id\"))\n",
    "df = df.with_columns(pl.coalesce(pl.col(\"id\").cast(pl.UInt64), 0).alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_by(\"id\", maintain_order=True).len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "start = 51000\n",
    "end = 51000 + 10000\n",
    "px.line(df[start : end], x=\"timestamp\", y=\"TP3\", color=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intentamos rellenar valores p칠rdidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "start = 51000\n",
    "end = 51000 + 10000\n",
    "test = pd.read_csv(\"../data/data.csv\")\n",
    "test = test.assign(TP3L = test.TP3.interpolate(method=\"linear\"))\n",
    "px.line(test[start : end], x=\"timestamp\", y=\"TP3L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendr칤amos que imputar solamente aquellos tramos cuya distancia fuera casi insignificante (algunos minutos). \n",
    "Ya que si observamos el Feb-9T00:00 a Feb-9T06:00 hemos introducido una l칤nea similar a lo que se콋ia una anomal칤a por el tama침o del salto temporal.\n",
    "\n",
    "Otra opci칩n ser칤a investigar introducir segmentos c칤clicos hasta que la diferencia fuera manejable. No obstante, esto ser칤a dif칤cil de argumentar y confuso para los posibles segmentos an칩malos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observar valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=1000):\n",
    "  print(df.filter(pl.col(\"timestamp\").gt(datetime.datetime(2020,2, 1, 19, 40, 00)) & pl.col(\"timestamp\").lt(datetime.datetime(2020, 2, 2, 6, 00, 00))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.col(\"timestamp\").diff().dt.total_seconds().gt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar valores raros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.filter(pl.col(\"timestamp\").is_between(*raros[5])), x=\"timestamp\", y=\"TP3\", color=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "\n",
    "df = pl.read_csv(\"../data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempo_medio_ciclo = 1260\n",
    "df.filter(pl.col(\"timestamp\").str.to_datetime().diff().dt.total_seconds() > tiempo_medio_ciclo * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creaci칩n del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "tiempo_medio_ciclo = 1260\n",
    "df = pl.read_csv(\"../data/data.csv\")\n",
    "# Str timestamp to datetime object\n",
    "df = df.select(pl.exclude(\"timestamp\"), pl.col(\"timestamp\").str.to_datetime())\n",
    "# Calculate distance from next read\n",
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))\n",
    "# Divide by mean cicle\n",
    "df = df.with_columns((pl.col(\"diff\").dt.total_seconds() // (tiempo_medio_ciclo * 2)).alias(\"id\"))\n",
    "# Count how many different samples are in each sliding sample \n",
    "df = df.join(df.group_by(\"id\").len(\"count\"), on=\"id\", how=\"inner\")\n",
    "# Filter those windows that have less than half points\n",
    "df = df.filter(pl.col(\"count\") >= (tiempo_medio_ciclo)/ (10))\n",
    "# Cast the ID column \n",
    "df = df.with_columns(pl.col(\"id\").cast(pl.UInt64).alias(\"id\"))\n",
    "# Asignamos la etiqueta\n",
    "df = df.select(pl.all(), pl.lit(0).alias(\"is_anomaly\"))\n",
    "for anomaly in final_anomalies:\n",
    "  df = df.with_columns((pl.col(\"is_anomaly\") + df[\"timestamp\"].is_between(anomaly[0], anomaly[1])))\n",
    "df = df.select(pl.exclude(\"is_anomaly\"), pl.col(\"is_anomaly\") >= 1)\n",
    "# Visualize a sample and the group length\n",
    "df.group_by(\"id\").first().with_columns(pl.col(\"count\").min().alias(\"smallest_window\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos las variables que vamos a utilizar.\n",
    "a = ['TP2',\n",
    "     'TP3',\n",
    "     'H1',\n",
    "     'DV_pressure',\n",
    "     'Reservoirs',\n",
    "     'Oil_temperature',\n",
    "     'Motor_current',\n",
    "     'COMP',\n",
    "     'DV_eletric',\n",
    "     'Towers',\n",
    "     'MPG',\n",
    "     'LPS',\n",
    "     'Pressure_switch',\n",
    "     'Oil_level',\n",
    "     'Caudal_impulses',\n",
    "]\n",
    "\n",
    "# Ahora obtenemos los diferentes ids que vamos a reducir (ventanas)\n",
    "df_X = df.select(\"id\").group_by(\"id\", maintain_order=True).first()\n",
    "\n",
    "# Calculamos las caracter칤sticas\n",
    "for c in a:\n",
    "  df_aux = df.group_by(\"id\", maintain_order=True).agg([\n",
    "      pl.col(f\"{c}\").mean().alias(f\"{c}_mean\"),\n",
    "      pl.col(f\"{c}\").max().alias(f\"{c}_max\"),\n",
    "      pl.col(f\"{c}\").min().alias(f\"{c}_min\"),\n",
    "      pl.col(f\"{c}\").median().alias(f\"{c}_median\"),\n",
    "      pl.col(f\"{c}\").var().alias(f\"{c}_var\"),\n",
    "  ])\n",
    "\n",
    "  df_X = df_X.join(df_aux, on=\"id\", how=\"left\")\n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetamos seg칰n mayor칤a en la ventana.\n",
    "labels = df.select([\"id\", \"is_anomaly\"]).group_by(pl.all()).len(\"count\")\n",
    "labels = labels.select(pl.all(), pl.col(\"count\").max().over(\"id\").alias(\"democracy\"))\n",
    "labels = labels.filter(pl.col(\"count\").eq(pl.col(\"democracy\"))).select([\"id\", \"is_anomaly\"])\n",
    "\n",
    "# Etiquetamos las anomal칤as\n",
    "df_X = df_X.join(labels, how=\"inner\", on=\"id\")\n",
    "df_X = df_X.drop(\"id\").with_row_index(\"id\")\n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 140421\n",
    "#Separamos anomalias de no anomalias\n",
    "df_no_anomaly = df_X.filter(pl.col(\"is_anomaly\") == False)\n",
    "df_anomaly = df_X.filter(pl.col(\"is_anomaly\") == True)\n",
    "\n",
    "#Agrupamos las que pertenecen a una misma anomalia\n",
    "df_anomaly = df_anomaly.sort(\"id\").with_columns(pl.col(\"id\").shift(1).fill_null(0).alias(\"last_id\"))\n",
    "\n",
    "df_anomaly = df_anomaly.with_columns(((pl.col(\"id\") - pl.col(\"last_id\")) >= 2).alias(\"group\"))\n",
    "\n",
    "df_anomaly = df_anomaly.with_columns(pl.col(\"group\").cum_sum())\n",
    "df_anomaly = df_anomaly.join(df_anomaly.select(\"id\", \"group\").group_by(\"group\", maintain_order=True).len(name=\"count\"),\n",
    "                             on=\"group\", how=\"inner\")\n",
    "\n",
    "#Creamos grupos de los grupos con un mismo tama침o para un k prefijado\n",
    "k = 9\n",
    "test_anomaly_size = int(df_anomaly.shape[0] / k)\n",
    "df_groups = df_anomaly.group_by(\"group\", maintain_order=True).first().select(\"group\", \"count\")\n",
    "df_groups = df_groups.sample(fraction=1, seed=seed, shuffle=True)\n",
    "\n",
    "groups = []\n",
    "for i in df_groups.to_dicts():\n",
    "    index = 0\n",
    "    flag_assigned = False\n",
    "    while index < len(groups) and not flag_assigned:\n",
    "        if groups[index][\"count\"] + i[\"count\"] < test_anomaly_size:\n",
    "            groups[index][\"group\"].append(i[\"group\"])\n",
    "            groups[index][\"count\"] += i[\"count\"]\n",
    "            flag_assigned = True\n",
    "            index = 0\n",
    "        else:\n",
    "            index += 1\n",
    "    if not flag_assigned and len(groups) >= index:\n",
    "        if len(groups) < k:\n",
    "            groups.append({\"count\": i[\"count\"], \"group\": [i[\"group\"]]})\n",
    "        else:\n",
    "            min_size = min([a[\"count\"] for a in groups])\n",
    "            for index in range(0, k):\n",
    "                if groups[index][\"count\"] == min_size:\n",
    "                    groups[index][\"group\"].append(i[\"group\"])\n",
    "                    groups[index][\"count\"] += i[\"count\"]\n",
    "                    flag_assigned = True\n",
    "                    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 140421\n",
    "#Repetimos el proceso para las no anomalias\n",
    "df_no_anomaly = df_no_anomaly.sort(\"id\").with_columns(pl.col(\"id\").shift(1).fill_null(0).alias(\"last_id\"))\n",
    "\n",
    "df_no_anomaly = df_no_anomaly.with_columns(((pl.col(\"id\") - pl.col(\"last_id\")) >= 2).alias(\"group\"))\n",
    "\n",
    "df_no_anomaly = df_no_anomaly.with_columns(pl.col(\"group\").cum_sum())\n",
    "df_no_anomaly = df_no_anomaly.join(df_no_anomaly.select(\"id\", \"group\").group_by(\"group\").len(name=\"count\"),\n",
    "                                   on=\"group\", how=\"inner\")\n",
    "\n",
    "#Creamos grupos de los grupos con un mismo tama침o para un k prefijado\n",
    "test_anomaly_size = int(df_no_anomaly.shape[0] / k)\n",
    "df_groups_no_anomaly = df_no_anomaly.group_by(\"group\", maintain_order=True).first().select(\"group\", \"count\")\n",
    "df_groups_no_anomaly = df_groups_no_anomaly.sample(fraction=1, seed=seed, shuffle=True)\n",
    "\n",
    "groups_no_anomaly = []\n",
    "for i in df_groups_no_anomaly.to_dicts():\n",
    "    index = 0\n",
    "    flag_assigned = False\n",
    "    while index < len(groups_no_anomaly) and not flag_assigned:\n",
    "        if groups_no_anomaly[index][\"count\"] + i[\"count\"] < test_anomaly_size:\n",
    "            groups_no_anomaly[index][\"group\"].append(i[\"group\"])\n",
    "            groups_no_anomaly[index][\"count\"] += i[\"count\"]\n",
    "            flag_assigned = True\n",
    "            index = 0\n",
    "        else:\n",
    "            index += 1\n",
    "    if not flag_assigned and len(groups_no_anomaly) >= index:\n",
    "        if len(groups_no_anomaly) < k:\n",
    "            groups_no_anomaly.append({\"count\": i[\"count\"], \"group\": [i[\"group\"]]})\n",
    "        else:\n",
    "            min_size = min([a[\"count\"] for a in groups_no_anomaly])\n",
    "            for index in range(0, k):\n",
    "                if groups_no_anomaly[index][\"count\"] == min_size:\n",
    "                    groups_no_anomaly[index][\"group\"].append(i[\"group\"])\n",
    "                    groups_no_anomaly[index][\"count\"] += i[\"count\"]\n",
    "                    flag_assigned = True\n",
    "                    index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0\n",
    "df_X = df_X.with_columns(pl.lit(None).alias(\"split_where_test\"))\n",
    "for a, b in zip(groups, groups_no_anomaly):\n",
    "    split_ids = df_anomaly.filter(pl.col(\"group\").is_in(a[\"group\"]))[\"id\"].to_list() + \\\n",
    "                df_no_anomaly.filter(pl.col(\"group\").is_in(b[\"group\"]))[\"id\"].to_list()\n",
    "    df_X = df_X.with_columns(\n",
    "        pl.when((pl.col(\"id\").is_in(split_ids))).then(split).otherwise(pl.col(\"split_where_test\")).alias(\"split_where_test\"))\n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.group_by(\"split_where_test\", \"is_anomaly\").len(name=\"count\").sort(\"split_where_test\", \"is_anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_X.filter(pl.col(\"split_where_test\").is_in([1,8]))\n",
    "df_test = df_test.select(pl.exclude([\"id\",\"split_where_test\"]))\n",
    "\n",
    "\n",
    "df_training = df_X.filter(~pl.col(\"split_where_test\").is_in([1,8]))\n",
    "df_training = df_training.with_columns(\n",
    "                                       pl.when(pl.col(\"split_where_test\").is_in([0,7]))\n",
    "                                       .then(0)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([2,6]))\n",
    "                                       .then(1)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([5,3]))\n",
    "                                       .then(2)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([4]))\n",
    "                                       .then(3).alias(\"split_where_test_2\"))\n",
    "df_training = df_training.select(pl.exclude([\"id\",\"split_where_test\"]))\n",
    "df_training = df_training.rename({\"split_where_test_2\": \"fold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.group_by(\"fold\", \"is_anomaly\").len(name=\"count\").sort(\"fold\", \"is_anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for a in groups:\n",
    "    if index not in [1,8,4]:\n",
    "        print(\"index: \",index ,\"count: \",a.get(\"count\"))\n",
    "    index += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.write_csv(\"../data/train.csv\")\n",
    "df_test.write_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "\n",
    "cv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(cv.cv_results_))\n",
    "print(cv.best_estimator_, cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = cv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(cv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance and Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "folds = train_data[fold_column].unique()\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "rcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb_var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "rcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(rcv.cv_results_))\n",
    "print(rcv.best_estimator_, rcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = rcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(rcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "folds = train_data[fold_column].unique()\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "rcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "rcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(rcv.cv_results_))\n",
    "print(rcv.best_estimator_, rcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = rcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(rcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRMR = [ \n",
    "  \"Towers_mean\", \n",
    "  \"DV_eletric_var\",\n",
    "  \"DV_pressure_median\",\n",
    "  \"TP2_max\",\n",
    "  \"Towers_median\",        \n",
    "  \"Reservoirs_max\",\n",
    "  \"DV_eletric_min\",       \n",
    "  \"Pressure_switch_min\", \n",
    "  \"Motor_current_var\",\n",
    "  \"Towers_var\",\n",
    "  \"LPS_min\",\n",
    "  \"DV_pressure_min\",\n",
    "  \"Towers_min\",\n",
    "  \"TP2_median\", \n",
    "  \"Oil_level_max\",\n",
    "  \"Reservoirs_mean\",\n",
    "  \"DV_pressure_mean\",\n",
    "  \"Oil_temperature_min\",\n",
    "  \"Oil_temperature_var\",  \n",
    "  \"DV_eletric_median\",\n",
    "  \"Caudal_impulses_max\",\n",
    "  \"DV_pressure_var\", \n",
    "  \"LPS_max\"              ,\n",
    "  \"H1_max\",\n",
    "  \"Motor_current_max\",\n",
    "  \"Motor_current_median\",\n",
    "  \"COMP_min\",\n",
    "  # \"TP2_var\",\n",
    "  # \"DV_eletric_mean\",      \n",
    "  # \"TP3_var\",\n",
    "  # \"Reservoirs_median\",\n",
    "  # \"H1_median\",\n",
    "]\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "\n",
    "mcv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "mcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(mcv.cv_results_))\n",
    "print(mcv.best_estimator_, mcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = mcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(mcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHI-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "x_train = MinMaxScaler().fit_transform(X_train) \n",
    "chi_scores = chi2(x_train, y_train)\n",
    "p_values = pd.Series(chi_scores[1],index = X_train.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values.plot.bar()\n",
    "plt.show()\n",
    "\n",
    "indexes = chi_scores[1] <= 0.05\n",
    "columns = X_train.columns\n",
    "print(columns[indexes])\n",
    "\n",
    "X_train = X_train.loc[:, indexes]\n",
    "\n",
    "ccv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "ccv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(ccv.cv_results_))\n",
    "print(ccv.best_estimator_, ccv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = ccv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(ccv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + ChiSquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "# feature_columns = MRMR\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "x_train = MinMaxScaler().fit_transform(X_train) \n",
    "chi_scores = chi2(x_train, y_train)\n",
    "\n",
    "indexes = chi_scores[1] <= 0.05\n",
    "columns = X_train.columns\n",
    "print(columns[indexes])\n",
    "\n",
    "\n",
    "X_train = X_train.loc[:, indexes]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour()),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "# feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour()),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train[MRMR])\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train[MRMR], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTELinks + MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "# feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train[MRMR])\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train[MRMR], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo de Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 140421\n",
    "# Utilizamos una versi칩n b치sica de stacking\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=seed)),\n",
    "    ('svc', SVC(random_state=seed)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "# Con un clasificador final lineal\n",
    "stack = StackingClassifier(\n",
    "  estimators=estimators,\n",
    "  final_estimator=LogisticRegression()\n",
    ")\n",
    "# Hacemos un GridSearch\n",
    "scv = GridSearchCV(\n",
    "  estimator = stack, \n",
    "  param_grid = {\n",
    "    # Decision Tree \n",
    "    'dt__max_depth': [None, 10],  # Unlimited or moderate depth\n",
    "    'dt__criterion': ['gini', 'entropy'],  # Gini or Entropy\n",
    "\n",
    "    # SVC\n",
    "    'svc__C': [0.1, 1, 10],  \n",
    "\n",
    "    # Naive Bayes\n",
    "    # 'nb__solver': [1e-9, 1e-8],  \n",
    "\n",
    "    # Logistic Regression\n",
    "    'final_estimator__penalty': [None, 'l2'], # Can focus on 1 model or be more moderate\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "scv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(scv.cv_results_))\n",
    "print(scv.best_estimator_, scv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = scv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients (assuming binary classification for simplicity)\n",
    "coefficients = scv.best_estimator_.final_estimator_.coef_.flatten()  # Flatten if multi-dimensional\n",
    "abs_coefficients = np.abs(coefficients)  # Get absolute values of coefficients\n",
    "total_weight = np.sum(abs_coefficients)  # Sum of absolute weights\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "data = {\n",
    "    \"Model Name\": [name for name, _ in stack.estimators],\n",
    "    \"Absolute Weight\": coefficients,\n",
    "    \"Relative Weight (%)\": (abs_coefficients / total_weight) * 100\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking + SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "seed = 140421\n",
    "# Utilizamos una versi칩n b치sica de stacking\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=seed)),\n",
    "    ('svc', SVC(random_state=seed)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "# Con un clasificador final lineal\n",
    "stack = StackingClassifier(\n",
    "  estimators=estimators,\n",
    "  final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "# Hacemos un GridSearch\n",
    "scv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid = {\n",
    "    # Decision Tree \n",
    "    'stack__dt__max_depth': [None, 10],  # Unlimited or moderate depth\n",
    "    'stack__dt__criterion': ['gini', 'entropy'],  # Gini or Entropy\n",
    "\n",
    "    # SVC\n",
    "    'stack__svc__C': [0.1, 1, 10],  \n",
    "\n",
    "    # Naive Bayes\n",
    "    # 'nb__solver': [1e-9, 1e-8],  \n",
    "\n",
    "    # Logistic Regression\n",
    "    'stack__final_estimator__penalty': [None, 'l2'], # Can focus on 1 model or be more moderate\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "scv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(scv.cv_results_))\n",
    "print(scv.best_estimator_, scv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = scv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients (assuming binary classification for simplicity)\n",
    "coefficients = scv.best_estimator_.named_steps[\"stack\"].final_estimator_.coef_.flatten() # Flatten if multi-dimensional\n",
    "abs_coefficients = np.abs(coefficients)  # Get absolute values of coefficients\n",
    "total_weight = np.sum(abs_coefficients)  # Sum of absolute weights\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "data = {\n",
    "    \"Model Name\": [name for name, _ in stack.estimators],\n",
    "    \"Absolute Weight\": coefficients,\n",
    "    \"Relative Weight (%)\": (abs_coefficients / total_weight) * 100\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaciones operaciones sobre test con los mejores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred = cv.best_estimator_.predict(X_test)\n",
    "print(pd.DataFrame(classification_report(y_test, ypred, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(cv.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred = scv.best_estimator_.predict(X_test)\n",
    "print(pd.DataFrame(classification_report(y_test, ypred, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
